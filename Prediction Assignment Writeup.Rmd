---
title: "Prediction Assignment Writeup"
author: "Rajesh Kolachana"
date: "April 3, 2016"
output: html_document
keep_md: yes
---
## Introduction
The activity data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants is recorded. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of this project is to use the available activity data to train the prediction model in order to accurately predict the barbell lift performed just by knowing the accerelometer readings.

## Data Loading
The train and test data sets are obtained by following the process below.
```{r echo=TRUE}
if (!file.exists("pml-training.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                destfile = "pml-training.csv")
}
if (!file.exists("pml-testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile = "pml-testing.csv")
}
```

## Preprocessing the data
Once the training data is obtained, the following operations are performed to preprocess the data set.
* The variables 1 to 7 in the data set which appeared non-meaningful for training the model are removed.
* The columns where 90% of the entries are missing are removed from the data set.
* The variables which don't contribute to classification are identified and removed from the data set.
* The remaining columns which contain missing values are imputed by fitting a bagged tree model for each predictor as a function of all the others. This method is simple, accurate and accepts missing values, but it has much higher computational cost.

The above operations are to be performed on both the training and the testing data sets, so they are embedded in a R function called `pProcess`.
```{r echo=TRUE}

# Function for preprocessing
pProcess <- function(dataframe){
  subsetTraining <- dataframe[,-(1:7)]; # manually remove non-meaningful columns
  
  end <- ncol(subsetTraining)           # get the index of outcome variable "classe"
  
  # coerce all variables but"classe" into numeric
  subsetTraining[,-end] <- data.frame(sapply(subsetTraining[,-end],as.numeric))
  
  # identify the columns missing more than 90% of their entries. 
  varsWith90NAs <- sapply(subsetTraining, function(vector){
    if(sum(is.na(vector))/length(vector) > 0.9){ # if vector is made of more than 90% NAs
      TRUE                             # return true
    }else{                            # if it doesn't
      FALSE                            # return false
    }
  })
  
  # remove the columns with more than 90% missing values
  subsetTraining <- subsetTraining[,!varsWith90NAs]
  
  # detect variables who don't contribute for the classification
  nzv <- nearZeroVar(subsetTraining[,-end],saveMetrics = TRUE)
  subsetTraining <- subsetTraining[,!as.logical(nzv$nzv)]
  
  if(any(is.na(subsetTraining))){               # if there are any remaining NAs
    # imput these missing values
    preProc <- preProcess(subsetTraining[,-end],method="bagImpute")
    subsetTraining[,-end] <- predict(preProc,subsetTraining[,-end])
    remove("preProc")                         # memory release    
  }
  invisible(subsetTraining)
}

library(caret)                              
set.seed(1234)                              # set random number generation seed
training <- read.csv("pml-training.csv", header=T, na.strings=c("NA", "#DIV/0!"))    # read training data
subsetTrainingIndex <- createDataPartition(training$classe, p=0.99, list = FALSE) #split into training and validation
subsetTraining <- training[subsetTrainingIndex,]

# preprocess the training dataset
traindata <- pProcess(subsetTraining)
```

## Model selection
Once the training data is preprocessed, a Random forest model is generated by performing a 5 fold cross validation. 
```{r echo=TRUE}
require(parallel)
require(doParallel)
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# model fit using random forests
system.time(modelFit <- train(classe ~ ., data = traindata, method="rf", trControl=trainControl(method="cv", number=5,                         allowParallel=T, verbose=T), verbose=F))

stopCluster(cl)

# the imporatance of the predictors for classification in decreasing order
varImportance <- varImp(modelFit)

```

## Evaluating the model
99% of the training data is used for training the prediction model and 1% is used for calculating the out of sample error to evaluate the model.
```{r echo=TRUE}
subsetTesting <- training[-subsetTrainingIndex,]
# preprocess it to get a tidy dataset
subsetTesting <- pProcess(subsetTesting)

# evaluate the model
errorMeasure <- confusionMatrix(subsetTesting$classe, predict(modelFit,subsetTesting))
errorMeasure  

outOfSampleError <- 1 - errorMeasure$overall[1]
names(outOfSampleError) <- "Out of Sample Error"
outOfSampleError

```
The above results show that the model is quite accurate in making the predictions.

## Predictions on test data set
The Random forest model constructed above is used to make the predictions on the test data set. The predictions are compared with the quiz results to obtain a 20/20 score.
```{r echo=TRUE}
testingFinal <- read.csv("pml-testing.csv")
testingFinal$classe <- 1:nrow(testingFinal)
testingFinal <- pProcess(testingFinal)

predict(modelFit,testingFinal)

```

